<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Quality metrics | Machine learning</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
          integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
    <meta name="google-site-verification" content="nKBS68MBglsXyUBNGGsQ7oQZU53wrBmnye0L6Fu4jBU"/>
    <script src="https://browser.sentry-cdn.com/4.6.5/bundle.min.js" crossorigin="anonymous"></script>
    <script type="text/javascript">Sentry.init({dsn: 'https://802d42edef034b74860743196de2cac2@sentry.io/1423759'});</script>
    <link rel="stylesheet"
          href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
</head>
<body>
<div class="container">
    <div class="py-5 text-center">
        <h1>Quality metrics</h1>
        <h4>Catalog / Best practices</h4>
    </div>
</div>
<div class="container">
    <div>
        <b>Problem definition.</b>
        <p>Machine learning algorithms predicts or measure something. But if we want to compare algorithms with another
            algorithms or with native benchmarks: random choice, last seen value or most common; we need to have quality
            functions which will compare predicted values with real values. </p>
        <p>Sometimes this terms splits into to metrics and losses.</p>
        <ul>
            <li><b>Metrics</b> is a function that is used to judge the performance of your model. It can not always be
                optimized directly.
            </li>
            <li><b>Losses</b> (or objective function, or optimization score function) is target of model optimization.
            </li>
        </ul>
        <p>Based on type and specificity of problem we can choose right quality metrics. One of the main problem is
            sensitivity of quality metric: do we want to detect small changes in performance or we need a robust
            measures. Another problem is difference between values in same sample: is standard deviation big or small.
            At the last we need to choose quality metric type: technical (usually sensitive but hard to explain) or
            business (robust and easy to explain and understand).</p>
    </div>
    <hr/>
    <div>
        <ul>
            <li><a href="#regression">Regression</a></li>
            <li><a href="#classification">Classification</a></li>
            <li><a href="#unsupervised">Unsupervised</a></li>
            <li><a href="#ranking">Ranking</a></li>
        </ul>
    </div>
    <hr/>
    <div>
        <h2><a href="#regression" name="regression">#</a> Regression</h2>
        <p>Regression problem called tasks that need to predict continues values of real numbers. "Age prediction"
            problem is one of the regression problem. Notice, when we need to predict independent labels like men/women
            in "Gender prediction" problem we are not solving regression problem.</p>

        <!-- MSE. BEGIN -->
        <h3>MSE. Mean squared error</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$${MSE} ={\frac {1}{n}}\sum _{i=1}^{n}(Y_{i}-{\hat {Y_{i}}})^{2}$$</span>
        <p>The MSE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero are
            better.</p>
        <!-- RMSE. END -->

        <!-- MAE. BEGIN -->
        <h3>MAE. Mean absolute error</h3>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$${MAE} ={\frac {\sum _{i=1}^{n}\left|y_{i}-x_{i}\right|}{n}}$$</span>
        <p>The MAE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero are
            better.</p>
        <!-- MAE. END -->

        <!-- MAE. BEGIN -->
        <h3>MedianAE. Median absolute error</h3>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$${MAE} =Median |y_{i}-x_{i}|$$</span>
        <p>The MedianAE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero
            are better.</p>
        <!-- MAE. END -->

        <!-- RMSLE. BEGIN -->
        <h3>RMSLE. Root mean squared logarithmic error</h3>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Technical</span>
        <span class="badge badge-secondary">Large values</span>

        <span>$$RMSLE = \sqrt{ \frac{1}{N} \sum_{i=1}^N (\log(x_i) - \log(y_i))^2 }$$</span>
        <p>The RMSLE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero
            are better.</p>
        <!-- RMSLE. END -->

        <!-- R2. BEGIN -->
        <h3>R^2. Coefficient of determination</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Business</span>
        <span class="badge badge-secondary">Bounded range (-inf, 1]</span>

        <span>$${\bar {y}}={\frac {1}{n}}\sum _{i=1}^{n}y_{i}; SS_{\text{tot}}=\sum _{i}(y_{i}-{\bar {y}})^{2}; SS_{\text{res}}=\sum _{i}(y_{i}-f_{i})^{2}$$</span>
        <span>$$R^{2}\equiv 1-{SS_{\rm {res}} \over SS_{\rm {tot}}}$$</span>
        <p>The R2 is a measure of the quality of an estimator — bounded range (-inf, 1]. 1 is best quality. 0 is mean
            target prediction. Higher is better.</p>
        <!-- R2. END -->

        <!-- MAPE. BEGIN -->
        <h3>MAPE. Mean absolute percentage error</h3>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Business</span>

        <span>$${MAPE}={\frac {100\%}{n}}\sum _{t=1}^{n}\left|{\frac {A_{t}-F_{t}}{A_{t}}}\right| $$</span>
        <!-- MAPE. END -->

        <!-- SMAPE. BEGIN -->
        <h3>SMAPE. Symmetric mean absolute percentage error</h3>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Business</span>

        <span>$${SMAPE}={\frac {100\%}{n}}\sum _{t=1}^{n}{\frac {\left|F_{t}-A_{t}\right|}{(|A_{t}|+|F_{t}|)/2}}$$</span>
        <p>Measure error as percent. It is fail when we have non-positive values.</p>
        <!-- SMAPE. END -->

        <h3>Weighed *</h3>
        <span class="badge badge-secondary">Sensitive+</span>
        <p>We always can add weight to each sample. If some samples is more important then one we can take this into
            account.</p>

        <h3>Asymmetric *</h3>
        <span class="badge badge-secondary">Sensitive+</span>
        <p>You can add asymmetric weight or shape to positive/negative errors.</p>
    </div>

    <hr/>

    <div>
        <h2><a href="#classification" name="classification">#</a> Classification</h2>
        <p>Classification problem called tasks that need to predict labels or probability of labels. If we have only two
            labels it is called binary classification problem, if more then two labels - multi-class classification
            problem. Bellow metrics described as binary but it can be easily modified to multi-classes with adding
            average or weighed average prefix.</p>
        <p>Example of classification problem is "Gender prediction".</p>

        <h3>Confusion matrix</h3>
        <table class="table">
            <tbody>
            <tr>
                <th colspan="2" rowspan="2"></th>
                <th colspan="3" class="text-center">Actual class</th>
            </tr>
            <tr>
                <th class="text-center">1</th>
                <th class="text-center">0</th>
            </tr>
            <tr>
                <th rowspan="3" class="text-right">
                    Predicted class
                </th>
                <th>1</th>
                <td class="text-center">True Positives [TP]</td>
                <td class="text-center">False Positives [FP] <span class="badge badge-secondary">Type I error</span>
                </td>
            </tr>
            <tr>
                <th>0</th>
                <td class="text-center">False Negatives [FN]<span class="badge badge-secondary">Type II error</span>
                </td>
                <td class="text-center">True Negatives [TN]</td>
            </tr>
            </tbody>
        </table>

        <div class="media">
            <img src="images/clf_errors_types.jpg" alt="I, II type of errors"/>
        </div>

        <h3>Based on probability</h3>

        <!-- ROC-AUC. BEGIN -->
        <h4>ROC-AUC</h4>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <p>This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds.
            It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you
            vary the threshold for assigning observations to a given class.</p>
        <!-- ROC-AUC. END -->

        <!-- GINI. BEGIN -->
        <h4>Gini</h4>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$$GINI=2*AUC - 1$$</span>
        <!-- GINI. BEGIN -->

        <!-- LOGLOSS. BEGIN -->
        <h4>LogLoss</h4>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$$LogLoss = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log{\hat{y_i}} + (1 - y_i) \log{(1-\hat{y_i})}]$$</span>
        <!-- LOGLOSS. END -->

        <h3>Based on labels</h3>

        <!-- Accuracy. BEGIN -->
        <h4>Accuracy</h4>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Business</span>

        <span>$$Accuracy=\frac{TP+TN}{TP+FN+FP+FN}$$</span>
        <!-- Accuracy. END -->

        <!-- PRECISION_RECALL. BEGIN -->
        <h4>Precision / Recall</h4>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Business</span>

        <span>$$Precision=\frac{TP}{TP+FP}$$</span>
        <span>$$Recall=\frac{TP}{TP+FN}$$</span>
        <!-- PRECISION_RECALL. END -->

        <!-- F1. BEGIN -->
        <h4>F1 score</h4>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$$F_1 = \frac{Precision * Recall}{Precision + Recall}$$</span>
        <!-- F1. END -->

        <!-- FBETA. BEGIN -->
        <h4>F-beta score</h4>
        <span class="badge badge-secondary">Robust</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$$F_{\beta} = (1+\beta^2)\frac{Precision * Recall}{\beta^2 * Precision + Recall}$$</span>
        <!-- FBETA. END -->

        <!-- COHEN KAPPA. BEGIN -->
        <h4>Cohen's kappa (quadratic weighted kappa)</h4>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>
        <!-- COHEN KAPPA. END -->

        <!-- Weighed. BEGIN -->
        <h4>Weighed *</h4>
        <span class="badge badge-secondary">Sensitive</span>

        <p>We always can add weight to each sample. If some samples is more important then one we can take this into
            account.</p>
        <!-- Weighed. END -->
    </div>
    <hr/>
    <div>
        <h2><a href="#classification" name="unsupervised">#</a> Unsupervised</h2>
        <p>In this case we train our model without knowledge about target values. When we evaluate performance we need
            to measure detected hidden patterns in data.</p>

        <!-- ARI. BEGIN -->
        <h3>ARI. Adjusted Rand Index</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>
        <span class="badge badge-secondary">Bounded range [-1, 1]</span>
        <span class="badge badge-secondary">Random (uniform) label assignments have a ARI score close to 0.0</span>
        <span class="badge badge-secondary">No assumption is made on the cluster structure</span>

        <p>If C is a ground truth class assignment and K the clustering, let us define and as:</p>
        <ul>
            <li>a, the number of pairs of elements that are in the same set in C and in the same set in K</li>
            <li>b, the number of pairs of elements that are in different sets in C and in different sets in K</li>
        </ul>
        <span>$$\text{RI} = \frac{a + b}{C_2^{n_{samples}}}$$</span>
        <span>$$ARI = \frac{RI - E_{RI}}{max(RI) - E_{RI}}$$</span>
        <p>Given the knowledge of the ground truth class assignments labels_true and our clustering algorithm
            assignments of the same samples labels_pred, the adjusted Rand index is a function that measures the
            similarity of the two assignments, ignoring permutations and with chance normalization.</p>
        <p>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the
            number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</p>
        <!-- ARI. END -->

        <!-- Silhouette Coefficient. BEGIN -->
        <h3>Silhouette Coefficient</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Business</span>
        <span class="badge badge-secondary">bounded between -1 for incorrect clustering and +1 for highly dense clustering</span>
        <span class="badge badge-secondary">score is higher when clusters are dense and well separated</span>

        <ul>
            <li>a: The mean distance between a sample and all other points in the same class.</li>
            <li>b: The mean distance between a sample and all other points in the next nearest cluster.</li>
        </ul>
        <span>$$s = \frac{b - a}{max(a, b)}$$</span>
        <p>Generally higher for convex clusters than other concepts of clusters, such as density based clusters like
            those obtained through DBSCAN</p>
        <!-- Silhouette Coefficient. END -->
    </div>
    <hr/>
    <div>
        <h2><a href="#ranking" name="ranking">#</a> Ranking</h2>

        <!-- MAP. BEGIN -->
        <h3>MAP. Mean average precision</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Business</span>

        <span>$${MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q}$$</span>
        <p>Mean average precision for a set of queries is the mean of the average precision scores for each query.</p>
        <!-- MAP. END -->

        <!-- DCG. BEGIN -->
        <h3>DCG. Discounted cumulative gain</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <p>The DCG accumulated at a particular rank position p is defined as:</p>
        <span>$$\mathrm {DCG_{p}} =\sum _{i=1}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}$$</span>

        <!-- NDCG. BEGIN -->
        <h3>NDCG. Normalized DCG.</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$$\mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}.$$</span>
        <!-- NDCG. END -->

        <!-- @K. BEGIN -->
        <h3>@K (Precision@K, NDCG@K)</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Business</span>

        <p>Evaluate quality on K samples with highest probability.</p>
        <!-- @K. END -->

        <h3>Mean reciprocal rank.</h3>
        <span class="badge badge-secondary">Sensitive</span>
        <span class="badge badge-secondary">Technical</span>

        <span>$$MRR={\frac{1}{|Q|}}\sum_{{i=1}}^{{|Q|}}{\frac  {1}{\text{rank}}_{i}}$$</span>
        <span>${\text{rank}}_{i}$ refers to the rank position of the first relevant document for the i-th query.</span>
    </div>

    <hr/>

    <div>
        <b>Related articles</b>:
        <ul>
            <li>
                <a href="https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4">Choosing
                    the Right Metric for Evaluating Machine Learning Models — Part 1</a>
            </li>
            <li>
                <a href="https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics">API Reference.
                    Sklearn metrics</a>
            </li>
            <li>
                <a href="https://alexanderdyakonov.files.wordpress.com/2018/10/book_08_metrics_12_blog1.pdf">Quality
                    metrics by Dyakonov [RU]</a>
            </li>
            <li>
                <a href="http://www.machinelearning.ru/wiki/images/f/fd/PZAD2017_03_errors.pdf">Error / quality
                    functions by Dyakonov [RU]</a>
            </li>
            <li>
                <a href="http://romip.ru/romip2009/15_yandex.pdf">Yandex pFound [RU]</a>
            </li>
        </ul>
    </div>
    <div class="text-right">
        <script type="text/javascript"
                src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5ca389f3b6700245"></script>
        <div class="addthis_inline_share_toolbox"></div>
    </div>
</div>
<hr/>
<div class="container">
    <div class="row">
        <div class="col-2"><img class="d-block mx-auto mb-4 rounded"
                                src="https://avatars1.githubusercontent.com/u/669347?s=100" alt=""
                                width="100"
                                height="100"></div>
        <div class="col-8">
            <p>Author <a href="https://mrkandreev.name/">@mrkandreev</a></p>
            <p>Machine Learning Engineer</p>
        </div>
    </div>
</div>
<script>hljs.initHighlightingOnLoad();</script>
<!-- Yandex.Metrika counter -->
<script type="text/javascript"> (function (d, w, c) {
    (w[c] = w[c] || []).push(function () {
        try {
            w.yaCounter41245659 = new Ya.Metrika2({
                id: 41245659,
                clickmap: true,
                trackLinks: true,
                accurateTrackBounce: true,
                webvisor: true,
                trackHash: true
            });
        } catch (e) {
        }
    });
    var n = d.getElementsByTagName("script")[0], s = d.createElement("script"), f = function () {
        n.parentNode.insertBefore(s, n);
    };
    s.type = "text/javascript";
    s.async = true;
    s.src = "https://mc.yandex.ru/metrika/tag.js";
    if (w.opera == "[object Opera]") {
        d.addEventListener("DOMContentLoaded", f, false);
    } else {
        f();
    }
})(document, window, "yandex_metrika_callbacks2"); </script>
<noscript>
    <div><img src="https://mc.yandex.ru/watch/41245659" style="position:absolute; left:-9999px;" alt=""/></div>
</noscript> <!-- /Yandex.Metrika counter -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css"
      integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js"
        integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij"
        crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js"
        integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
        crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        });
    });
</script>
</body>
</html>